# ğŸ§  Local-Embedding RAG System

A privacy-first, production-grade Retrieval-Augmented Generation (RAG) system built using local embeddings, a persistent vector database, and GitHub Models (Azure Inference) for grounded answer generation.

This project demonstrates how to build a cost-efficient, scalable, and offline-friendly RAG pipeline without relying on paid embedding APIs.

---

## ğŸš€ Features

- ğŸ“š Retrieval-Augmented Generation (RAG)
- ğŸ” Semantic search using vector similarity
- ğŸ§  100% local embeddings (SentenceTransformers â€“ no OpenAI embedding cost)
- ğŸ—„ Persistent vector storage using ChromaDB
- ğŸŒ Web page ingestion via `urls.txt`
- ğŸ“„ Local document ingestion (.txt files)
- ğŸ§© Smart chunking with overlap
- â™»ï¸ Deterministic duplicate-safe indexing (SHA-256 hashing)
- ğŸ¯ Similarity threshold gating (reduces hallucinations)
- ğŸ“Œ Source attribution for answers
- ğŸ–¥ Multiple UI options (CLI / Tkinter / Streamlit)

---

## ğŸ— System Architecture

### 1ï¸âƒ£ Data Ingestion Layer
- Local `.txt` file loader
- Web scraping (Requests + BeautifulSoup4)
- HTML cleaning (removes `<script>`, `<style>`, `<noscript>`)
- URL retry logic

### 2ï¸âƒ£ Processing & Embedding Layer
- Chunk size: **200**
- Overlap: **50**
- Embedding model: **all-MiniLM-L6-v2**
- Framework: **PyTorch (CPU compatible)**

### 3ï¸âƒ£ Vector Storage Layer
- **ChromaDB (Persistent)**
- Cosine similarity search
- On-disk storage
- Incremental indexing support

### 4ï¸âƒ£ Retrieval & Generation Layer
- Top-K semantic retrieval
- Similarity threshold: **0.40**
- Context-only prompting
- LLM: **GPT-4o-mini (GitHub Models via Azure Inference SDK)**

---

## ğŸ“ Project Structure

rag_app/
â”œâ”€â”€ app.py
â”œâ”€â”€ ui.py
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ chroma_db/
â”œâ”€â”€ documents/
â”‚ â”œâ”€â”€ urls.txt
â”‚ â””â”€â”€ *.txt
â””â”€â”€ .env


---

## âš™ï¸ Prerequisites

- Python 3.10+
- GitHub account with GitHub Models access
- Internet (for URL ingestion only)

---

## ğŸ” Environment Setup

Create a `.env` file:

GITHUB_TOKEN=your_github_personal_access_token


Get token from:
https://github.com/marketplace/models

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate        # Linux/macOS
venv\Scripts\activate           # Windows

pip install -r requirements.txt

pip install torch --index-url https://download.pytorch.org/whl/cpu

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install CPU PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cpu

ğŸ“„ Adding Documents

Place .txt files inside:

documents/


Add URLs inside:

documents/urls.txt


Rules:

One URL per line

Blank lines allowed

Lines starting with # ignored

ğŸ§± Index Data
python app.py --index


This:

Loads documents

Chunks text

Generates embeddings

Stores vectors in ChromaDB

â“ Ask Questions
CLI
python app.py

Tkinter UI
python ui.py

Streamlit UI
streamlit run streamlit_app.py

ğŸ§  Hallucination Control

Similarity threshold filtering

Strict context-only prompting

If insufficient context:

"I don't have enough information to answer that."

ğŸ“Š Performance Characteristics

âš¡ Fast local embedding generation

ğŸ’° Zero embedding API cost

ğŸ“ˆ Scalable retrieval via persistent vector DB

ğŸ”’ No document data sent externally during indexing

ğŸ›  Tech Stack

Python 3.10+

ChromaDB

SentenceTransformers

PyTorch

GitHub Models (Azure Inference)

BeautifulSoup4

Requests

Tkinter

Streamlit


---

# âœ… Now Finish the Rebase

Run:

```bash
git add rag_app/README.md
git rebase --continue
git push origin dev


This keeps:

Your professional architecture description

Your UI updates

Your earlier structured explanations

Clean formatting

No conflict markers
